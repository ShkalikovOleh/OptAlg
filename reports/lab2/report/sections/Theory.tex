\begin{center}
    \section*{Теоретична частина}
\end{center}
\SetKwInOut{Parametr}{}

Задача безумовної оптимізації скалярної функції $f(x)$
полягає у тому, щоб знайти такий $x_{min} \in D$
($ D \subseteq \mathbb{R}^n $ - область визначення $f$), що:

\begin{equation} \label{eq:optimization_task}
    x_{min} = \argmin\limits_{x \in D} f(x)
\end{equation}

Не для усіх функцій можливо просто та швидко
аналітично знайти екстремуми, тому у практичних
задачах дуже часто використовуються числені методи
оптимізації. У даній роботі ми розглянемо методи другого порядку,
методи Нелдера-Міда, Хука-Дживса та генетичний алгоритм.

\subsection*{Метод Нелдера-Міда}

Розглянемо один з найвідоміших методів нерегулярного симплекса,
а саме метод Нелдера-Міда. Він полягає у тому, що на кожному кроці
алгоритма ми певним чином деформуємо сиплекс(многогранник
$S \in \mathbb{R}^{n}$, де $n$-розмірність задачі).
Подамо усі можливі деформації ($x_c = \dfrac{\sum\limits_{i=1}^{n} x_i}{n}$):
\begin{enumerate}
    \item Reflection($\alpha > 0 \in \mathbb{R}$):
    $ x_r = x_c + \alpha(x_c - x_{n+1}) $
    \item Expansion($\gamma > 0 \in \mathbb{R}$):
    $ x_e = x_c + \gamma (x_r - x_c) $
    \item Contraction($0 < \rho \leq \frac{1}{2} \in \mathbb{R}$):
    $ x_{contr} = x_c + \rho (x_r - x_c) $
    \item Shrink($i \neq 1$, $0 < \sigma  \leq \frac{1}{2} \in \mathbb{R}$): $x_i = x_1 + \sigma(x-i - x_1)$
\end{enumerate}
Порядок застосування данних деформацій наведемо на діаграмі нижче:

% \usetikzlibrary{shapes.geometric, arrows}
% \tikzstyle{startstop} = [rectangle, rounded corners, text centered, draw=black, fill=red!30]
% \tikzstyle{process} = [rectangle, text centered, draw=black, fill=orange!30]
% \tikzstyle{decision} = [rectangle, rounded corners, text width = 2.55cm, text centered, draw=black, fill=green!30]
% \tikzstyle{arrow} = [thick,->,>=stealth]

% \begin{figure}[h!]
%     \begin{center}
%         \begin{tikzpicture}[node distance=1.5cm]
%             \node[startstop] (start) at(-8,-2) {start};
%             \node[decision] (criterion) at (-5,-2) {Виконується критерій зупинки?};

%             \node[process] (final_sort) at(-5,-4.7) {Sort};
%             \node[startstop] (stop) at(-5,-6.5) {$x_{min} = x_1$};

%             \node[process] (sort) at (-1, -2) {Sort};
%             \node[process] (reflection) at (2, -2) {Reflection};
%             \node[decision] (ref_check) at (5.5, -2) {$f(x_1)\leq f(x_r)$ $f(x_r)<f(x_n)$};

%             \node[process] (contraction) at (5.5, 0.5) {Contraction};
%             \node[process] (expansion) at (5.5, -4.7) {Expansion};

%             \node[decision] (contr_check) at (2, 0.5) {$f(x_c) < f(x_{n+1})$};
%             \node[decision] (exp_check) at (2, -4.7) {$f(x_e) < f(x_r)$};

%             \node[process] (shrink) at (-1, 0.5) {Shrink};

%             \draw[arrow] (start)--(criterion);

%             \draw[arrow] (criterion)--node[anchor=east]{так} (final_sort);
%             \draw[arrow] (final_sort)--(stop);

%             \draw[arrow] (criterion)--node[anchor=south]{ні} (sort);
%             \draw[arrow] (sort)--(reflection);

%             \draw[arrow] (reflection)--(ref_check);
%             \draw[arrow] (ref_check)--node[anchor=east]{так} ++(0,-1.5)
%             --node[anchor=south]{$x_{n+1}=x_r$} ++(-8,0)--(criterion);

%             \draw[arrow] (ref_check)--++(2,0)--
%             node[anchor=west, yshift=-1.5cm, xshift=-0.3cm, rotate=90]{$f(x_r) < f(x_1)$}
%             ++(0,-2.7)--(expansion);
%             \draw[arrow] (ref_check)--++(2,0)--
%             node[anchor=west, yshift=-1.5cm, xshift=0.3cm, rotate=90]{$f(x_r) \geq f(x_n)$}
%             ++(0,2.5)--(contraction);

%         \end{tikzpicture}
%     \end{center}
%     \caption{Блок-схема метода Нелдера-Міда}
%     \label{fig:nelder_mead}
% \end{figure}

Отже, отримаємо алгоритм метода Нелдера-Міда.

\SetKw{Continue}{continue}
\begin{algorithm}[H] \label{alg:nelder_mead}
    \SetAlgoLined
    \KwIn{$ S_n = \{x_1 \dots\, x_{n+1}\}, \alpha, \gamma, \rho, \sigma $}
    \KwResult{$x_{min}$}
    \While{не виконана умова зупинки}
    {
        sort($S_n$)\;
        $ x_c \leftarrow \dfrac{\sum\limits_{i=1}^{n} x_i}{n} $ \;

        $ x_r  \leftarrow x_c + \alpha(x_c - x_{n+1}) $ \tcp{Reflection}
        \If{$f(x_1) \leq f(x_r) < f(x_n)$}
        {
            $x_{n+1} \leftarrow x_r$ \;
            \Continue
        }

        \If{$f(x_r) < f(x_1)$}
        {
            $x_e \leftarrow x_c + \gamma (x_r - x_c)$ \tcp{Expansion}
            \If{$f(x_e) < f(x_r)$}{
                $x_{n+1} \leftarrow x_e$ \;
            }
            \Else{
                $x_{n+1} \leftarrow x_r$ \;
            }
            \Continue
        }

        \If{$f(x_r) \geq f(x_n)$}
        {
            $x_{contr} \leftarrow x_c + \rho (x_r - x_c)$ \tcp{Contraction}
            \If{$f(x_{contr}) < f(x_{n+1})$}{
                $x_{n+1} \leftarrow x_{contr}$ \;
                \Continue
            }
        }


        \For{$1 < i \leq n+1$}
        {
            $x_i \leftarrow x_1 + \sigma(x-i - x_1)$ \tcp{Shrink}
        }
    }
    sort($S_n$)\;
    \KwOut{$x_1$}
    \caption{Метод Нелдера-Міда}
\end{algorithm}

\vspace*{0.5cm}
У якості критерія зупинки можна обрати наступні умови:
\begin{enumerate}
    \item Кількість ітерації вища за деяке наперед задане $k \in \mathbb{N}$
    \item Стандартне відхилення значення функції у кожній з точок
    симплекса менше за деяке $\varepsilon > 0 \in \mathbb{R}$.
\end{enumerate}

\subsection*{Методи спуску}

Для оптимізації скалярної функцій можна
використовувати методи спуску, що на кожному кроці
будуть наближати нас до якогось локального мінімуму.
Їх можна описати наступною процедурої зміни
аргумента функції($\alpha_k > 0$, $k \in \mathbb{N}$):

\begin{equation} \label{eq:descent}
    x_{k+1} = x_{k} - \alpha_k p_{k}
\end{equation}
За умови, що виконується нерівність:
\begin{equation} \label{eq:descent_require}
    f(x_{k+1}) < f(x_k)
\end{equation}

Ітерації спуску продовжуются доки не виконана умова зупинки.
Серед умов зупинки можна виділити наступні($\varepsilon > 0,\;
n \in \mathbb{N}$ - параметри):

\begin{enumerate}
    \item Значення градієнту: $|\nabla f_k| < \varepsilon$
    \item Різниця аргументів: $|x_k - x_{k-1}| < \varepsilon$
    \item Різниця значення функції: $|f(x_k) - f(x_{k-1})| < \varepsilon$
    \item Кількість ітерацій більша або рівна за $n$
\end{enumerate}
Де $ \nabla f_{k} = \nabla f(x_{k})$(надалі будемо
використовувати це позначення).

\subsubsection*{Метод Гука-Дживса}

Метод Гука-Дживса є методом прямого пошуку, що використовує
для визначення напряму спуску окрему процедуру вичерпуючого
пошуку(exploratory-search). А саме, для даної точки $x$ та
вектору "змін" $b$ ми змінюємо кожну координату за правилом
(спочатку $x_b = x_k$):

$$ x_{b_i} = x_{k_i} + b_i \;,\text{якщо}
    \begin{cases}
        f(x_{b_1}, \dots x_{k_i} + b_i \dots ,x_{k_n}) < f(x_k) \\
        f(x_{b_1}, \dots x_{k_i} + b_i \dots ,x_{k_n}) \leq f(x_{b_1}, \dots x_{k_i} - b_i \dots ,x_{k_n})
    \end{cases}
$$
$$ x_{b_i} = x_{k_i} - b_i \;,\text{якщо}
f(x_{b_1}, \dots x_{k_i} - b_i \dots ,x_{k_n}) < f(x_k)
$$
$$ x_{b_i} = x_{k_i} \;,\text{якщо}
\begin{cases}
    f(x_{b_1}, \dots x_{k_i} + b_i \dots ,x_{k_n}) \geq f(x_k) \\
    f(x_{b_1}, \dots x_{k_i} - b_i \dots ,x_{k_n}) \geq f(x_k)
\end{cases}
$$

Напрям спуску обирається 2 способами:
\begin{enumerate}
    \item (Pattern move) Якщо рух за напрямком з попереднього кроку
    зменшує нашу функцію, то ми переходимо у цю точку та вже з неї
    вичерпуючим пошуком знаходимо нову точку. Цей варіант руху допомагає
    значно прискорити спуск до точки мінімуму.
    \item Якщо pattern move не вдався ми намагаємось вичерпуючим
    пошуком з поточної точки знайти нову. Якщо це не вдається, то
    ми зменшуємо вектор "змін" помноживши його на $\gamma \in (0,1)$.
    Кількість таких зменшень $t$ є параметром алгоритма, і якщо
    нової точки знайдено не було, то алгоритм завершує роботу.
\end{enumerate}

Величину $\alpha$(step size) можна обирати за допомогою методів
оптимізації функції однієї змінної, або ж взяти за константу.
Автори цього методу рекомендують обирати $\alpha = 2$.
Таким чином, об'єднуючи усе це отримаємо алгоритм Гука-Дживса.

\pagebreak
\SetKw{Break}{break}
\begin{algorithm}[H] \label{alg:hooke_jeeves}
    \SetAlgoLined
    \KwIn{$x_0$, $b$, $\gamma$, $t$}
    \KwResult{$x_{min}$}
    $x_1 \leftarrow x_0$\;
    \While{не виконана умова зупинки}
    {
        \If(pattern move){$f(x_k + \alpha (x_k - x_{k-1})) < f(x_k)$}
        {
            $x^{'}_k \leftarrow \text{exploratory-search} (x_k + \alpha (x_k - x_{k-1}), b)$ \;
            $x_{k+1} \leftarrow x_k + \alpha (x^{'}_{k} - x_{k})$ \;
        }
        \Else
        {
            $j = 0$ \;
            $x^{i}_{k} \leftarrow x_k$ \;
            \While{$(t > j) \wedge (f(x^{'}_k) > f(x_k)) $}
            {
                $x^{'}_k \leftarrow \text{exploratory-search} (x_k, b \cdot \gamma^{j})$ \;
                $ j \leftarrow j + 1 $ \;
            }
            \If{$t == j$}
            {
            \Break
            }
            \Else
            {
                $ x_{k+1} \leftarrow x^{'}_k $ \;
            }
        }
    }
    \KwOut{$x_k$}
    \caption{Метод Гука-Дживса}
\end{algorithm}

\subsubsection*{Метод Ньютона}

Метод Ньютона являє собою методом спуску, що використовує
похідні другого цільової функцій, а саме, напрям спуску
процедури \ref{eq:descent} має вигляд:
\begin{equation}
    p_k = H^{-1}_k \nabla f_k
\end{equation} \label{eq:newton}
,де $H_k$ - матриця Гессе функції $f$ у точці $x_k$.
Такий вибір кроку забезпечує більшу швидкість збіжності у
порівнянні з методами, що використовують лише похідні першого порядку,
бо більш враховує властивості цільової функції.
Отже, маємо алгоритм метода Ньютона.

\begin{algorithm}[H] \label{alg:newton}
    \SetAlgoLined
    \KwIn{$x_0$}
    \KwResult{$x_{min}$}
    $x_1 \leftarrow x_0$\;
    \While{не виконана умова зупинки}
    {
        $\alpha_k \leftarrow \argmin\limits_{\alpha > 0} f(x_{k} - \alpha H^{-1}_k \nabla f_{k})$\;
        $x_{k+1} \leftarrow x_{k} - \alpha_k H^{-1}_k \nabla f_{k}$\;
    }
    \KwOut{$x_k$}
    \caption{Метод Ньютона}
\end{algorithm}

\subsubsection*{Квазіньютоновські методи}

Метод Ньютона вимагає позитивну визначенність матриці Гессе
цільової функції у точці на кожному кроці. Ця вимога значно обмежує
кількість функцій, які ми можемо оптимізувати за допомогою
цього методу. Тому, квазіньютоновські методи використовують
у процесі своєї роботи не обернений гессіан,
а наближенне його значення $B_k$:
\begin{equation}
    p_k = B_k \nabla f_k
\end{equation}
Існує багато методів обчислення цього наближення $B_k$.
Серед них можна виділити наступні
($y_k = \nabla f_k - \nabla f_{k-1}, \;
\Delta x_k = x_k - x_{k-1} $):
\begin{enumerate}
    \item SR1
    $$ B_k = B_{k-1} + \frac{(\Delta x_{k} - B_{k-1}y_k)
    (\Delta x_{k} - B_{k-1}y_k)^{T}}
    {(\Delta x_{k} - B_{k-1}y_k)^{T}y_k} $$
    \item Broyden
    $$ B_k = B_{k-1} + \frac{(\Delta x_{k} - B_{k-1}y_k)
    \Delta x_{k}^{T}B_{k-1}}
    {\Delta x_{k}^{T}B_{k-1}y_k} $$
    \item DFP
    $$ B_k = B_{k-1} + \frac{\Delta x_{k}\Delta x_{k}^{T}}
    {\Delta x_{k}^{T}y_k} - \frac{B_{k-1}y_ky_k^{T}B_{k-1}}{y_k^{T}B_{k-1}y_k} $$
    \item BFGS
    $$ B_k = \left( I - \frac{\Delta x_{k}y_k^{T}}{y_k^{T}\Delta x_k} \right) B_{k-1}
    \left( I - \frac{y_k \Delta x_{k}^{T}}{y_k^{T}\Delta x_k} \right) +
    \frac{\Delta x_{k}\Delta x_{k}^{T}}{y_k^{T}\Delta x_k}
     $$
\end{enumerate}

Таким чином усі квазіньютоновські алгоритми можна подати у наступному
вигляді.

\begin{algorithm}[H] \label{alg:newton}
    \SetAlgoLined
    \KwIn{$x_0$}
    \KwResult{$x_{min}$}
    $x_1 \leftarrow x_0$\;
    \While{не виконана умова зупинки}
    {
        $\alpha_k \leftarrow \argmin\limits_{\alpha > 0} f(x_{k} - \alpha B_k \nabla f_{k})$\;
        $x_{k+1} \leftarrow x_{k} - \alpha_k B_k \nabla f_{k}$\;
    }
    \KwOut{$x_k$}
    \caption{Квазіньютоновські методи}
\end{algorithm}

\subsection*{Генетичний алгоритм}

Генетичні алгоритми (ГА) -- це процедури пошуку, що базуються на механізмах природного добору та спадковості.
У них використовується еволюційний принцип виживання найбільш пристосованих особин.

Головні поняття генетичних алгоритмів:
\begin{enumerate}
    \item \emph{Популяція} -- це скінченна множина особин.
    \item \emph{Особини}, що складають популяцію, у ГА представлені хромосомами із закодованими в них
    точками у просторі пошуку (search points).
    \item \emph{Хромосоми (ланцюжки, кодові послідовності)} -- впорядковані послідовності генів.
    \item \emph{Ген} -- атомарний елемент генотипу.
    \item \emph{Генотип (структура)} -- набір хромосом особини.
    \item \emph{Фенотип} -- набір значень, що відповідає генотипу, тобто декодована структура.
    \item \emph{Алель} -- значення конкретного гена.
    \item \emph{Локус (позиція)} вказує місце розташування гена в хромосомі.
    Множина позицій генів -- це локи.
\end{enumerate}

Іншим важливим поняттям у ГА є \emph{фукція пристосованості (fitness function)}
(у задачах оптимізації це цільова функція).

\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, text width=5cm, minimum height=1cm,text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, text width=2cm, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=5cm, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

\begin{figure}
    \begin{center}
        \begin{tikzpicture}[node distance=2cm]
            \node (start) [startstop] {START};
            \node (step1) [process, below of=start, yshift=-0.35cm] {Ініціалізація - формування початкової популяції хромосом};
            \node (step2) [process, below of=step1, yshift=-0.5cm] {Обчислення пристосованості хромосом у популяції};
            \node (criterion) [decision, below of=step2, yshift=-1.5cm] {Виконана умова зупинки?};
            \node (step3) [process, right of=criterion, xshift=4.5cm] {Вибір найкращої хромосоми};
            \node (stop) [startstop, below of=step3, yshift=-0.35cm] {STOP};
            \node (step4) [process, below of=criterion, yshift=-1.8cm] {Селекція хромосом};
            \node (step5) [process, below of=step4, yshift=-0.35cm] {Застосування генетичних операторів};
            \node (step6) [process, below of=step5, yshift=-0.35cm] {Формування нової популяції};

            \draw [arrow] (start) -- (step1);
            \draw [arrow] (step1) -- (step2);
            \draw [arrow] (step2) -- (criterion);
            \draw [arrow] (criterion) -- node[anchor=east] {ні} (step4);
            \draw [arrow] (criterion) -- node[anchor=south] {так} (step3);
            \draw [arrow] (step3) -- (stop);
            \draw [arrow] (step4) -- (step5);
            \draw [arrow] (step5) -- (step6);
            \draw [arrow] (step6.west) -- ++(-3,0) -- ++(0,12) -- (step2);
        \end{tikzpicture}
    \end{center}
    \caption{Блок-схема генетичного алгоритму}
    \label{fig:GA_flow}
\end{figure}

\subsubsection*{Класичний генетичний алгоритм}
На рис.\ref{fig:GA_flow} приведено блок-схему класичного генетичного алгоритму.
Роздивимось конкретні кроки більш детально:

\begin{enumerate}
    \item \textbf{Ініціалізація} -- формування початкової популяції шляхом генерації випадкових хромосом
    заданої кількості, що представлені двійковими послідовностями фіксованої довжини.
    \item \textbf{Оцінка пристосованості} -- обчислення функції пристосованості для кожної хромосоми цієї популяції.
    Очікується, що функція невід'ємна на проміжку, що досліджується, та необхідно знайти максимум.
    \item \textbf{Перевірка умови зупинки алгоритму} -- перевіряється, чи виконалась умова зупинки (досягнення певної точності,
    відсутність покращення результату, максимальний час або кількість ітерацій).
    \item \textbf{Селекція} -- вибір тих хромосом, що будуть брати участь у створенні нащадків для наступної популяції
    (відповідно до значень функції пристосованості).
    Найбільш популярним є \emph{метод рулетки (roulette wheel selection)}.
    Кожній хромосомі співставляється сектор колеса рулетки, розмір якого пропорційний до значення функції пристосованості
    даної хромосоми, а все колесо рулетки відповідає сумі значень функції пристосованості всіх хромосом у популяції.
    Тоді ймовірність <<влучити>> у відповідний для кожної хромосоми $ch_i$ сектор обчислюється за формулою:

    \[p_s(ch_i) = \frac{F(ch_i)}{\displaystyle\sum_{j=0}^{N} F(ch_j)}\]
    де $F(ch_i)$ -- це значення функції пристосованості хромосоми $ch_i$, а $N$ -- розмір популяції.
    У результаті селекції формується батьківський пул (mating pool) того ж розміру, що й популяція.
    \item \textbf{Застосування генетичних операторів} призводить до формування нової популяції нащадків.
    У класичному ГА використовують два оператори: \emph{оператор схрещування (кросинговеру) (crossover)} та
    \emph{оператор мутації (mutation)}.
    При цьому ймовірність схрещування значно вища ($0.5 \leq p_c \leq 1$) за ймовірність мутації ($0 \leq p_m \leq 0.1$).

    \textbf{Оператор схрещування}.
    На першому етапі обираються пари хромосом із батьківської популяції, що отримана в результаті селекції.
    Це відбувається випадковим чином відповідно до ймовірності схрещування $p_c$.
    Далі випадково обирається локус, що визначає так звану точку схрещування
    (якщо хромосоми мають довжину $L$, то очевидно, що точка схрещування $1 \leq l_k \leq L-1$).
    У результаті схрещування отримуємо наступну пару нащадків:
    \begin{enumerate}
        \item нащадок, хромосома якого на позиціях від $1$ до $l_k$ складається з генів першої батьківської хромосоми, а
        на позиціях від $l_k+1$ до $L$ -- з генів другої хромосоми.
        \item нащадок, хромосома якого на позиціях від $1$ до $l_k$ складається з генів другої батьківської хромосоми, а
        на позиціях від $l_k+1$ до $L$ -- з генів першої хромосоми.
    \end{enumerate}
    Також можливе багатоточкове схрещування, тоді обирається декілька точок схрещування та ділянки чергуються.

    \textbf{Оператор мутації} із ймовірністю $p_m$ змінює значення гена в хромосомі на протилежне.
    \item \textbf{Формування нової популяції}.
    У класичному ГА вся попередня популяція хромосом замінюється новою популяцією нащадків того ж розміру.
    \item \textbf{Вибір <<найкращої>> хромосоми}.
    Після виконання умови зупинки в якості результату роботи алгоритму повертається хромосома з найбільшим значенням функції пристосованості.
\end{enumerate}