\begin{center}
    \section*{Теоретична частина}
\end{center}
\SetKwInOut{Parametr}{}

Задача безумовної оптимізації скалярної функції $f(x)$
полягає у тому, щоб знайти такий $x_{min} \in D$
($ D \subseteq \mathbb{R}^n $ - область визначення $f$), що:

\begin{equation} \label{eq:optimization_task}
    x_{min} = \argmin\limits_{x \in D} f(x)
\end{equation}

Не для усіх функцій можливо просто та швидко
аналітично знайти екстремуми, тому у практичних
задачах дуже часто використовуються числені методи
оптимізації. У даній роботі ми розглянемо

\subsection*{Метод Нелдера-Міда}

\subsection*{Методи спуску}

Для оптимізації скалярної функцій можна
використовувати методи спуску, що на кожному кроці
будуть наближати нас до якогось локального мінімуму.
Їх можна описати наступною процедурої зміни
аргумента функції($\alpha_k > 0$, $k \in \mathbb{N}$):

\begin{equation} \label{eq:descent}
    x_{k+1} = x_{k} - \alpha_k p_{k}
\end{equation}
За умови, що виконується нерівність:
\begin{equation} \label{eq:descent_require}
    f(x_{k+1}) < f(x_k)
\end{equation}

Ітерації спуску продовжуются доки не виконана умова зупинки.
Серед умов зупинки можна виділити наступні($\varepsilon > 0,\;
n \in \mathbb{N}$ - параметри):

\begin{enumerate}
    \item Значення градієнту: $|\nabla f_k| < \varepsilon$
    \item Різниця аргументів: $|x_k - x_{k-1}| < \varepsilon$
    \item Різниця значення функції: $|f(x_k) - f(x_{k-1})| < \varepsilon$
    \item Кількість ітерацій більша або рівна за $n$
\end{enumerate}
Де $ \nabla f_{k} = \nabla f(x_{k})$(надалі будемо
використовувати це позначення).

\subsection*{Метод Гука-Дживса}

\subsection*{Метод Ньютона}

Метод Ньютона являє собою методом спуску, що використовує
похідні другого цільової функцій, а саме, напрям спуску
процедури \ref{eq:descent} має вигляд:
\begin{equation}
    p_k = H^{-1}_k \nabla f_k
\end{equation} \label{eq:newton}
,де $H_k$ - матриця Гессе функції $f$ у точці $x_k$.
Такий вибір кроку забезпечує більшу швидкість збіжності у
порівнянні з методами, що використовують лише похідні першого порядку,
бо більш враховують властивості цільової функції.
Отже, маємо алгоритм метода Ньютона.

\begin{algorithm}[H] \label{alg:newton}
    \SetAlgoLined
    \KwIn{$x_0$}
    \KwResult{$x_{min}$}
    $x_1 \leftarrow x_0$\;
    \While{не виконана умова зупинки}
    {
        $\alpha_k \leftarrow \argmin\limits_{\alpha > 0} f(x_{k} - \alpha H^{-1}_k \nabla f_{k})$\;
        $x_{k+1} \leftarrow x_{k} - \alpha_k H^{-1}_k \nabla f_{k}$\;
    }
    \KwOut{$x_k$}
    \caption{Метод Ньютона}
\end{algorithm}

\subsection*{Квазіньютоновські методи}

Метод Ньютона вимагає позитивну визначенність матриці Гессе
цільової функції у точці на кожному кроці. Ця вимога значно обмежує
Кількість функцій, які ми можемо оптимізувати за допомогою
цього методу. Тому, квазіньютоновські методи використовують
у процесі своєї роботи не обернений гессіан(як у \ref{eq:newton}),
а наближенне його значення:
\begin{equation}
    p_k = B_k \nabla f_k
\end{equation}
Існує багато методів обчислення цього наближення $B_k$.
Серед них можна виділити наступні
($y_k = \nabla f_k - \nabla f_{k-1}, \;
\Delta x_k = x_k - x_{k-1} $):
\begin{enumerate}
    \item SR1
    $$ B_k = B_{k-1} + \frac{(\Delta x_{k} - B_{k-1}y_k)
    (\Delta x_{k} - B_{k-1}y_k)^{T}}
    {(\Delta x_{k} - B_{k-1}y_k)^{T}y_k} $$
    \item Broyden
    $$ B_k = B_{k-1} + \frac{(\Delta x_{k} - B_{k-1}y_k)
    \Delta x_{k}^{T}B_{k-1}}
    {\Delta x_{k}^{T}B_{k-1}y_k} $$
    \item DFP
    $$ B_k = B_{k-1} + \frac{\Delta x_{k}\Delta x_{k}^{T}}
    {\Delta x_{k}^{T}y_k} - \frac{B_{k-1}y_ky_k^{T}B_{k-1}}{y_k^{T}B_{k-1}y_k} $$
    \item BFGS
    $$ B_k = \left( I - \frac{\Delta x_{k}y_k^{T}}{y_k^{T}\Delta x_k} \right) B_{k-1}
    \left( I - \frac{y_k \Delta x_{k}^{T}}{y_k^{T}\Delta x_k} \right) +
    \frac{\Delta x_{k}\Delta x_{k}^{T}}{y_k^{T}\Delta x_k}
     $$
\end{enumerate}

Таким чином усі квазіньютоновські алгоритми можна подати у наступному
вигляді.

\begin{algorithm}[H] \label{alg:newton}
    \SetAlgoLined
    \KwIn{$x_0$}
    \KwResult{$x_{min}$}
    $x_1 \leftarrow x_0$\;
    \While{не виконана умова зупинки}
    {
        $\alpha_k \leftarrow \argmin\limits_{\alpha > 0} f(x_{k} - \alpha B_k \nabla f_{k})$\;
        $x_{k+1} \leftarrow x_{k} - \alpha_k B_k \nabla f_{k}$\;
    }
    \KwOut{$x_k$}
    \caption{Квазіньютоновські методи}
\end{algorithm}

\subsection*{Генетичний алгоритм}

Генетичні алгоритми (ГА) -- це процедури пошуку, що базуються на механізмах природного добору та спадковості.
У них використовується еволюційний принцип виживання найбільш пристосованих особин.

Головні поняття генетичних алгоритмів:
\begin{enumerate}
    \item \emph{Популяція} -- це скінченна множина особин.
    \item \emph{Особини}, що складають популяцію, у ГА представлені хромосомами із закодованими в них
    точками у просторі пошуку (search points).
    \item \emph{Хромосоми (ланцюжки, кодові послідовності)} -- впорядковані послідовності генів.
    \item \emph{Ген} -- атомарний елемент генотипу.
    \item \emph{Генотип (структура)} -- набір хромосом особини.
    \item \emph{Фенотип} -- набір значень, що відповідає генотипу, тобто декодована структура.
    \item \emph{Алель} -- значення конкретного гена.
    \item \emph{Локус (позиція)} вказує місце розташування гена в хромосомі.
    Множина позицій генів -- це локи.
\end{enumerate}

Іншим важливим поняттям у ГА є \emph{фукція пристосованості (fitness function)}
(у задачах оптимізації це цільова функція).

\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, text width=5cm, minimum height=1cm,text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, text width=2cm, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=5cm, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, text width=2cm, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

\begin{figure}
    \begin{center}
        \begin{tikzpicture}[node distance=2cm]
            \node (start) [startstop] {START};
            \node (step1) [process, below of=start, yshift=-0.35cm] {Ініціалізація - формування початкової популяції хромосом};
            \node (step2) [process, below of=step1, yshift=-0.5cm] {Обчислення пристосованості хромосом у популяції};
            \node (criterion) [decision, below of=step2, yshift=-1.5cm] {Виконана умова зупинки?};
            \node (step3) [process, right of=criterion, xshift=4.5cm] {Вибір найкращої хромосоми};
            \node (stop) [startstop, below of=step3, yshift=-0.35cm] {STOP};
            \node (step4) [process, below of=criterion, yshift=-1.8cm] {Селекція хромосом};
            \node (step5) [process, below of=step4, yshift=-0.35cm] {Застосування генетичних операторів};
            \node (step6) [process, below of=step5, yshift=-0.35cm] {Формування нової популяції};

            \draw [arrow] (start) -- (step1);
            \draw [arrow] (step1) -- (step2);
            \draw [arrow] (step2) -- (criterion);
            \draw [arrow] (criterion) -- node[anchor=east] {ні} (step4);
            \draw [arrow] (criterion) -- node[anchor=south] {так} (step3);
            \draw [arrow] (step3) -- (stop);
            \draw [arrow] (step4) -- (step5);
            \draw [arrow] (step5) -- (step6);
            \draw [arrow] (step6.west) -- ++(-3,0) -- ++(0,12) -- (step2);
        \end{tikzpicture}
    \end{center}
    \caption{Блок-схема генетичного алгоритму}
    \label{fig:GA_flow}
\end{figure}

\subsubsection*{Класичний генетичний алгоритм}
На рис.\ref{fig:GA_flow} приведено блок-схему класичного генетичного алгоритму.
Роздивимось конкретні кроки більш детально:

\begin{enumerate}
    \item \textbf{Ініціалізація} -- формування початкової популяції шляхом генерації випадкових хромосом
    заданої кількості, що представлені двійковими послідовностями фіксованої довжини.
    \item \textbf{Оцінка пристосованості} -- обчислення функції пристосованості для кожної хромосоми цієї популяції.
    Очікується, що функція невід'ємна на проміжку, що досліджується, та необхідно знайти максимум.
    \item \textbf{Перевірка умови зупинки алгоритму} -- перевіряється, чи виконалась умова зупинки (досягнення певної точності,
    відсутність покращення результату, максимальний час або кількість ітерацій).
    \item \textbf{Селекція} -- вибір тих хромосом, що будуть брати участь у створенні нащадків для наступної популяції
    (відповідно до значень функції пристосованості).
    Найбільш популярним є \emph{метод рулетки (roulette wheel selection)}.
    Кожній хромосомі співставляється сектор колеса рулетки, розмір якого пропорційний до значення функції пристосованості
    даної хромосоми, а все колесо рулетки відповідає сумі значень функції пристосованості всіх хромосом у популяції.
    Тоді ймовірність <<влучити>> у відповідний для кожної хромосоми $ch_i$ сектор обчислюється за формулою:

    \[p_s(ch_i) = \frac{F(ch_i)}{\displaystyle\sum_{j=0}^{N} F(ch_j)}\]
    де $F(ch_i)$ -- це значення функції пристосованості хромосоми $ch_i$, а $N$ -- розмір популяції.
    У результаті селекції формується батьківський пул (mating pool) того ж розміру, що й популяція.
    \item \textbf{Застосування генетичних операторів} призводить до формування нової популяції нащадків.
    У класичному ГА використовують два оператори: \emph{оператор схрещування (кросинговеру) (crossover)} та
    \emph{оператор мутації (mutation)}.
    При цьому ймовірність схрещування значно вища ($0.5 \leq p_c \leq 1$) за ймовірність мутації ($0 \leq p_m \leq 0.1$).

    \textbf{Оператор схрещування}.
    На першому етапі обираються пари хромосом із батьківської популяції, що отримана в результаті селекції.
    Це відбувається випадковим чином відповідно до ймовірності схрещування $p_c$.
    Далі випадково обирається локус, що визначає так звану точку схрещування
    (якщо хромосоми мають довжину $L$, то очевидно, що точка схрещування $1 \leq l_k \leq L-1$).
    У результаті схрещування отримуємо наступну пару нащадків:
    \begin{enumerate}
        \item нащадок, хромосома якого на позиціях від $1$ до $l_k$ складається з генів першої батьківської хромосоми, а
        на позиціях від $l_k+1$ до $L$ -- з генів другої хромосоми.
        \item нащадок, хромосома якого на позиціях від $1$ до $l_k$ складається з генів другої батьківської хромосоми, а
        на позиціях від $l_k+1$ до $L$ -- з генів першої хромосоми.
    \end{enumerate}
    Також можливе багатоточкове схрещування, тоді обирається декілька точок схрещування та ділянки чергуються.

    \textbf{Оператор мутації} із ймовірністю $p_m$ змінює значення гена в хромосомі на протилежне.
    \item \textbf{Формування нової популяції}.
    У класичному ГА вся попередня популяція хромосом замінюється новою популяцією нащадків того ж розміру.
    \item \textbf{Вибір <<найкращої>> хромосоми}.
    Після виконання умови зупинки в якості результату роботи алгоритму повертається хромосома з найбільшим значенням функції пристосованості.
\end{enumerate}